############################################################################
# This section `secrets` is for sharing secret files between the host
# and Docker containers.  Unlike volumes, secrets can be outside the
# project's directory structure and have special rules to help prevent them
# from being included in a Docker image.
#
# A common development use case is ssh keys for private git repositories.
############################################################################
secrets:
  claude-auth:
    file: ~/.claude/.credentials.json
  claude-cfg:
    file: ~/.claude.json

#  gemini-auth:
#    file: ~/.gemini
#  ssh:
#    file: ~/.ssh
#  aws:
#    file: ~/.aws


############################################################################
# This section `services` is where the "servers" are.  Each service provides
# a "server".  Common development use cases are web servers, database
# servers and testing servers.
#
# The purpose of each service is to define how the service (aka server)
# interacts with the host.  So 8080:80 means localhost:8080 is mapped
# to 80 inside the service.
#
# Services that have "build" are modified locally from a Dockerfile. Think
# of them as "customizing" what is installed on the "server".
#
# Services that have "image" are used in a default service configuration.
#
# The "port" maps a TCP/IP port from the service to the host (real computer).
#
# The "volumes" map a directory from the host to inside the service.
#
# All the services in the file share a private common network and can talk
# to each other over that private network using the service name as the
# host name.
############################################################################
services:

  wordpress:
    build:
      context: .
      dockerfile: .docker/WordPress.Dockerfile
    restart: always
    ports:
      # SECURITY: Remove or change to "127.0.0.1:8080:80" after initial setup
      # This port bypasses the NPM proxy and should not be exposed in production
      - 8080:80
    environment:
      WORDPRESS_DB_HOST: wordpress-db
      WORDPRESS_DB_USER: ${WORDPRESS_DB_USER}
      WORDPRESS_DB_PASSWORD: ${WORDPRESS_DB_PASSWORD}
      WORDPRESS_DB_NAME: ${WORDPRESS_DB_NAME}
    volumes:
      - ./wordpress:/var/www/html
      - ./logs/wordpress:/var/log/apache2
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/"]
      interval: 5m
      timeout: 10s
      retries: 3
      start_period: 60s

  wordpress-db:
    image: mysql:8.0
    restart: always
    environment:
      MYSQL_DATABASE: ${WORDPRESS_DB_NAME}
      MYSQL_USER: ${WORDPRESS_DB_USER}
      MYSQL_PASSWORD: ${WORDPRESS_DB_PASSWORD}
#      MYSQL_RANDOM_ROOT_PASSWORD: '1'
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
    volumes:
      - ./wordpress-db:/var/lib/mysql
      - ./logs/mysql:/var/log/mysql
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "root", "-p${MYSQL_ROOT_PASSWORD}"]
      interval: 5m
      timeout: 10s
      retries: 3
      start_period: 60s

  npm:
    image: 'jc21/nginx-proxy-manager:latest'
    restart: unless-stopped
    environment:
      TZ: ${TZ}
    ports:
      - '9080:80'
      # SECURITY: Remove or change to "127.0.0.1:81:81" after initial setup
      # This is the NPM admin interface and should not be exposed in production
      - '9081:81'
      - '9443:443'
    volumes:
      - ./npm/data:/data
      - ./npm/letsencrypt:/etc/letsencrypt
      - ./logs/npm:/data/logs
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:81/"]
      interval: 5m
      timeout: 10s
      retries: 3
      start_period: 60s

  ##########################################################################
  # LLM Inference Server (llama.cpp)
  # Provides OpenAI-compatible API for local LLM inference
  # API endpoint: http://localhost:8091/v1/chat/completions
  ##########################################################################
  llama-server:
    build:
      context: .
      dockerfile: .docker/llamacpp.Dockerfile

    ports:
      - "8191:8091"

    volumes:
      - ./models:/models

    # For debugging, uncomment to keep container running:
    # command: bash -c 'sleep infinity'

    command: >
      /opt/llama.cpp/build/bin/llama-server
      -m /models/granite-4.0-h-tiny-Q4_K_M.gguf
      -mu https://huggingface.co/ibm-granite/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q4_K_M.gguf?download=true
      -c 4096
      --host 0.0.0.0
      --port 8091
      --mlock

    # ========================================================================
    # llama-server Command Line Options Reference
    # ========================================================================
    #
    # MODEL OPTIONS:
    #   -m, --model FILE        Path to the model file (required)
    #   -mu, --model-url URL    Download model from URL if not present locally
    #   -a, --alias NAME        Set model alias for API responses
    #
    # CONTEXT OPTIONS:
    #   -c, --ctx-size N        Context size (default: 4096, 0 = model default)
    #   -n, --predict N         Max tokens to predict (default: -1, -1 = infinite)
    #   --keep N                Tokens to keep from initial prompt (default: 0)
    #
    # GPU OPTIONS:
    #   -ngl, --gpu-layers N    Layers to offload to GPU (default: 0, 999 = all)
    #   -sm, --split-mode       GPU split mode: none, layer, row (default: layer)
    #   -mg, --main-gpu N       Main GPU for scratch/small tensors (default: 0)
    #   -ts, --tensor-split     Comma-separated list of GPU tensor split ratios
    #
    # PERFORMANCE OPTIONS:
    #   -t, --threads N         CPU threads for generation (default: auto)
    #   -tb, --threads-batch N  CPU threads for batch processing (default: auto)
    #   -b, --batch-size N      Logical batch size (default: 2048)
    #   -ub, --ubatch-size N    Physical batch size (default: 512)
    #   --mlock               Force model to stay in RAM
    #   --no-mmap             Don't memory-map the model
    #
    # SERVER OPTIONS:
    #   --host HOST             Listen address (default: 127.0.0.1, use 0.0.0.0 for all)
    #   --port PORT             Listen port (default: 8080)
    #   --timeout N             Server timeout in seconds (default: 600)
    #   -np, --parallel N       Max parallel sequences (default: 1)
    #   --cont-batching         Enable continuous batching (default: disabled)
    #
    # GENERATION OPTIONS:
    #   --temp N                Temperature (default: 0.8)
    #   --top-k N               Top-K sampling (default: 40)
    #   --top-p N               Top-P sampling (default: 0.9)
    #   --repeat-penalty N      Repeat penalty (default: 1.1)
    #   --seed N                Random seed (default: -1 = random)
    #
    # API OPTIONS:
    #   --api-key KEY           API key for authentication (optional)
    #   --chat-template NAME    Chat template: llama2, chatml, etc.
    #   --embeddings            Enable embedding endpoint
    #
    # LOGGING OPTIONS:
    #   -v, --verbose           Enable verbose output
    #   --log-disable           Disable logging
    #   --log-file FILE         Log to file instead of stderr
    #
    # EXAMPLE CONFIGURATIONS:
    #
    # High performance (GPU with 24GB VRAM):
    #   -m /models/model.gguf -c 8192 -ngl 999 --cont-batching -np 4
    #
    # CPU only with more threads:
    #   -m /models/model.gguf -c 4096 -t 8 --mlock
    #
    # Low memory usage:
    #   -m /models/model.gguf -c 2048 -b 512 --no-mmap
    #
    # ========================================================================

  agents:
    build:
      context: .
      dockerfile: .docker/TypeScript.Dockerfile
      target: ai-dev

    # This section sets the environment variables inside the Docker container.
    environment:
      - TZ=America/Chicago # see https://en.wikipedia.org/wiki/List_of_tz_database_time_zones

      # Application environment
      - APP_ENV=development
      - NODE_ENV=development

      # Server configuration
      - PORT=8001
      - BASE_URL=http://localhost:8001
      - AGENT_CONFIG_PATH=./agents

      # Google AI API Key - passed from host environment or .env file
      - GOOGLE_GENERATIVE_AI_API_KEY=${GOOGLE_GENERATIVE_AI_API_KEY}

    ports:
      - "8001:8001"

    volumes:
      - ./:/project

    working_dir: /project/agents

    # Start the agents framework with hot-reload
    command: bash -c 'npm install && npm run dev'

    # Allow access to agent service and other services
    extra_hosts:
      - "host.docker.internal:host-gateway"

  ##########################################################################
  # AI Development Service
  # Container for LLM agent to develop and test clippy-chat
  # Accessible via: docker exec -it wordpress-ai-dev-1 bash
  ##########################################################################
  ai-dev:

    # This section tells Docker to build a new container using the
    # setup specified in the given dockerfile.
    build:
      context: .
      dockerfile: .docker/TypeScript.Dockerfile
      target: ai-dev

    # This section sets the environment variables inside the Docker container.
    environment:
      - TZ=America/Chicago # see https://en.wikipedia.org/wiki/List_of_tz_database_time_zones

      # Application environment
      - APP_ENV=development

      # Agent API access (internal)
      - AGENT_API_URL=http://agent:8088

      # Qwen
      - OPENAI_API_KEY=your_api_key_here
      - OPENAI_BASE_URL=http://host.docker.internal:12434/engines/llama.cpp/v1
      - OPENAI_MODEL=ai/qwen3-coder:latest

      # Google AI API Key - passed from host environment or .env file
      - GOOGLE_GENERATIVE_AI_API_KEY=${GOOGLE_GENERATIVE_AI_API_KEY}

    # Exposed ports for clippy-chat development and testing
    # {host port}:{container port}
    #    ports:
    #      - "3000:3000"

    # `volumes` tells Docker to map folders and files from the host to
    # inside the Docker container.
    # NOTE: The host folder/file MUST be inside the project directory.
    # {host folder}:{container folder}
    volumes:
      - ./:/project
      # Special CLAUDE.md file so we know we are inside docker
      - ./docker.CLAUDE.md:/home/user/.claude/CLAUDE.md
      # Mount Docker in Docker
      - /var/run/docker.sock:/var/run/docker.sock

    working_dir: /project/

    # Sleep infinity allows agent to work interactively inside container
    command: bash -c 'sleep infinity'

    # Allow access to agent service and other services
    extra_hosts:
      - "host.docker.internal:host-gateway"

    # The `secrets` allows folders and files specified in the `secrets` at the top of this
    # file to be mapped inside the container.  To use git and composer in the container,
    # you will need to map your ssh key.
    secrets:
      #      - source: gemini-auth
      #        target: /home/user/.gemini.system
      #
      - source: claude-auth
        target: /home/user/.claude.system/.credentials.json
        uid: "1000"
        gid: "1000"
        mode: 0600
      - source: claude-cfg
        target: /home/user/.claude.json.system
        uid: "1000"
        gid: "1000"
        mode: 0600
  #
  #      - source: ssh
  #        target: /home/user/.ssh
  #        uid: "1000"
  #        gid: "1000"
  #        mode: 0700

  #      - source: aws
  #        target: /home/user/.aws
  #        uid: "1000"
  #        gid: "1000"
  #        mode: 0700