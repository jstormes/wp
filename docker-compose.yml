
############################################################################
# This section `services` is where the "servers" are.  Each service provides
# a "server".  Common development use cases are web servers, database
# servers and testing servers.
#
# The purpose of each service is to define how the service (aka server)
# interacts with the host.  So 8080:80 means localhost:8080 is mapped
# to 80 inside the service.
#
# Services that have "build" are modified locally from a Dockerfile. Think
# of them as "customizing" what is installed on the "server".
#
# Services that have "image" are used in a default service configuration.
#
# The "port" maps a TCP/IP port from the service to the host (real computer).
#
# The "volumes" map a directory from the host to inside the service.
#
# All the services in the file share a private common network and can talk
# to each other over that private network using the service name as the
# host name.
############################################################################
services:

  wordpress:
    build:
      context: .
      dockerfile: .docker/WordPress.Dockerfile
    restart: always
    ports:
      # SECURITY: Remove or change to "127.0.0.1:8080:80" after initial setup
      # This port bypasses the NPM proxy and should not be exposed in production
      - 8080:80
    environment:
      WORDPRESS_DB_HOST: wordpress-db
      WORDPRESS_DB_USER: ${WORDPRESS_DB_USER}
      WORDPRESS_DB_PASSWORD: ${WORDPRESS_DB_PASSWORD}
      WORDPRESS_DB_NAME: ${WORDPRESS_DB_NAME}
    volumes:
      - ./wordpress:/var/www/html
      - ./logs/wordpress:/var/log/apache2
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/"]
      interval: 5m
      timeout: 10s
      retries: 3
      start_period: 60s

  wordpress-db:
    image: mysql:8.0
    restart: always
    environment:
      MYSQL_DATABASE: ${WORDPRESS_DB_NAME}
      MYSQL_USER: ${WORDPRESS_DB_USER}
      MYSQL_PASSWORD: ${WORDPRESS_DB_PASSWORD}
#      MYSQL_RANDOM_ROOT_PASSWORD: '1'
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
    volumes:
      - ./wordpress-db:/var/lib/mysql
      - ./logs/mysql:/var/log/mysql
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "root", "-p${MYSQL_ROOT_PASSWORD}"]
      interval: 5m
      timeout: 10s
      retries: 3
      start_period: 60s

  npm:
    image: 'jc21/nginx-proxy-manager:latest'
    restart: unless-stopped
    environment:
      TZ: ${TZ}
    ports:
      - '9080:80'
      # SECURITY: Remove or change to "127.0.0.1:81:81" after initial setup
      # This is the NPM admin interface and should not be exposed in production
      - '9081:81'
      - '9443:443'
    volumes:
      - ./npm/data:/data
      - ./npm/letsencrypt:/etc/letsencrypt
      - ./logs/npm:/data/logs
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:81/"]
      interval: 5m
      timeout: 10s
      retries: 3
      start_period: 60s

  ##########################################################################
  # LLM Inference Server (llama.cpp)
  # Provides OpenAI-compatible API for local LLM inference
  # API endpoint: http://localhost:8091/v1/chat/completions
  ##########################################################################
  llama-server:
    build:
      context: .
      dockerfile: .docker/llamacpp.Dockerfile

    ports:
      - "8191:8091"

    volumes:
      - ./models:/models

    # For debugging, uncomment to keep container running:
    # command: bash -c 'sleep infinity'

    command: >
      /opt/llama.cpp/build/bin/llama-server
      -m /models/granite-4.0-h-tiny-Q4_K_M.gguf
      -mu https://huggingface.co/ibm-granite/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q4_K_M.gguf?download=true
      -c 4096
      --host 0.0.0.0
      --port 8091
      --mlock

    # ========================================================================
    # llama-server Command Line Options Reference
    # ========================================================================
    #
    # MODEL OPTIONS:
    #   -m, --model FILE        Path to the model file (required)
    #   -mu, --model-url URL    Download model from URL if not present locally
    #   -a, --alias NAME        Set model alias for API responses
    #
    # CONTEXT OPTIONS:
    #   -c, --ctx-size N        Context size (default: 4096, 0 = model default)
    #   -n, --predict N         Max tokens to predict (default: -1, -1 = infinite)
    #   --keep N                Tokens to keep from initial prompt (default: 0)
    #
    # GPU OPTIONS:
    #   -ngl, --gpu-layers N    Layers to offload to GPU (default: 0, 999 = all)
    #   -sm, --split-mode       GPU split mode: none, layer, row (default: layer)
    #   -mg, --main-gpu N       Main GPU for scratch/small tensors (default: 0)
    #   -ts, --tensor-split     Comma-separated list of GPU tensor split ratios
    #
    # PERFORMANCE OPTIONS:
    #   -t, --threads N         CPU threads for generation (default: auto)
    #   -tb, --threads-batch N  CPU threads for batch processing (default: auto)
    #   -b, --batch-size N      Logical batch size (default: 2048)
    #   -ub, --ubatch-size N    Physical batch size (default: 512)
    #   --mlock               Force model to stay in RAM
    #   --no-mmap             Don't memory-map the model
    #
    # SERVER OPTIONS:
    #   --host HOST             Listen address (default: 127.0.0.1, use 0.0.0.0 for all)
    #   --port PORT             Listen port (default: 8080)
    #   --timeout N             Server timeout in seconds (default: 600)
    #   -np, --parallel N       Max parallel sequences (default: 1)
    #   --cont-batching         Enable continuous batching (default: disabled)
    #
    # GENERATION OPTIONS:
    #   --temp N                Temperature (default: 0.8)
    #   --top-k N               Top-K sampling (default: 40)
    #   --top-p N               Top-P sampling (default: 0.9)
    #   --repeat-penalty N      Repeat penalty (default: 1.1)
    #   --seed N                Random seed (default: -1 = random)
    #
    # API OPTIONS:
    #   --api-key KEY           API key for authentication (optional)
    #   --chat-template NAME    Chat template: llama2, chatml, etc.
    #   --embeddings            Enable embedding endpoint
    #
    # LOGGING OPTIONS:
    #   -v, --verbose           Enable verbose output
    #   --log-disable           Disable logging
    #   --log-file FILE         Log to file instead of stderr
    #
    # EXAMPLE CONFIGURATIONS:
    #
    # High performance (GPU with 24GB VRAM):
    #   -m /models/model.gguf -c 8192 -ngl 999 --cont-batching -np 4
    #
    # CPU only with more threads:
    #   -m /models/model.gguf -c 4096 -t 8 --mlock
    #
    # Low memory usage:
    #   -m /models/model.gguf -c 2048 -b 512 --no-mmap
    #
    # ========================================================================